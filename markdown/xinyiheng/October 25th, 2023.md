- 
- Imported Quick Capture items:
    - 直观理解！你一定要读一下的“矩阵和线性代数入门” - 知乎
        - Annotations:

* 矩阵的最初目的，就是线性方程组的简写！

* 范数的引入是很自然的，我们描述一个“数”的大小，用“数值”，描述一个向量的长度（欧式距离），用“欧氏距离”，那么如果我想描述矩阵的“大小”（姑且这样说）该用什么？

用到的就是范数。



https://zhuanlan.zhihu.com/p/137112358 [Email Body](https://files.todoist.com/MoHl58Vqxgrv4XYGbPMZOhFfha9IW3ndd7RonDDp5hpW4vHCDu3P5IHpLSDPYI4k/by/21878347/as/file.html)
    - ChatGPT
        - Annotations:

*     * 感觉就是找到方差较大的某一类数据，尽量保留，而其他变动比较小的某些类型的数据，则不用每个都保留

*     1. 什么是PCA? 当我们有很多数据特征（就像前面提到的苹果的颜色、大小、重量等）时，有时候很多特征之间是有关联的，这就意味着它们可能有重复的信息。PCA就是一种方法，帮我们把这些重复或者相似的信息整合起来，只保留最关键的部分，从而减少数据的维度。

2. 怎么工作的?

* 方差和信息: 在统计学中，一个数据特征的方差（变化的大小）经常被认为是包含信息的量。大的方差意味着数据在这个特征上有很大的变化，可能包含很多信息。而小的方差则表示这个特征上的数据都很相似，没有太多信息。

* 找到主成分: PCA的目标是找到一个新的角度来看数据，这个角度能让数据展现出最大的方差。这个角度就是我们说的“主成分”。想象一下，如果你从地面看一本书，你可能只能看到书的封面；但如果你从上面看，你会看到书的厚度。在这个例子中，PCA就是帮我们找到从哪个角度看书可以得到最多的信息。

* 降维: 一旦我们找到了第一个主成分（也就是最重要的角度），我们可以再找第二个、第三个，等等。但这些新的角度都要和前面的主成分垂直。最后，我们可能会决定只保留前几个主成分，因为它们包含了数据的大部分信息，而丢弃其他的，这样就实现了数据降维。

3. 为什么要降维? 数据的维度越多，处理起来就越复杂，需要的时间和计算资源也越多。通过降维，我们可以让数据变得更简单，更易于处理，同时也减少了计算成本。而PCA是一种常用的降维方法，因为它能帮助我们找到数据中最重要的信息。

总的来说，PCA就是一个找到数据中最关键信息，然后用更少的特征来表示这些信息的方法。



https://chat.openai.com/c/cd6f0411-cec2-4b16-8f4d-46d90fa48742 [Email Body](https://files.todoist.com/OHV9RWgYb7qGEqr5NQFXzhld4lldVRKcPItHmQruZaAWbwuvpJ9xvaVZnYx5BAhb/by/21878347/as/file.html)
