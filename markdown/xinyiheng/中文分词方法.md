- [ 文本挖掘的分词原理 - 云+社区 - 腾讯云 ](https://cloud.tencent.com/developer/article/1087047)
    - 分词的基本原理
        - ^^现代分词都是基于统计的分词^^，而统计的样本内容来自于一些标准的语料库。假如有一个句子：“小明来到荔湾区”，我们期望语料库统计后分词的结果是："小明/来到/荔湾/区"，而不是“小明/来到/荔/湾区”。那么如何做到这一点呢？从统计的角度，我们期望"小明/来到/荔湾/区"这个分词后句子出现的概率要比“小明/来到/荔/湾区”大。
    - [ 分词原理 ](https://www.diigo.com/outliner/diigo_items/904019/12128769/573677859?key=34d57b46e1)
    - 一元模型
        - 在NLP中，为了简化计算，我们通常使用[[马尔科夫假设]]，即每一个分词出现的概率仅仅和前一个分词有关。
    - N元模型  
        - 当然，你会说，只依赖于前一个词太武断了，我们能不能依赖于前两个词呢？即：  我们一般称只依赖于前一个词的模型为二元模型(Bi-Gram model)，而依赖于前两个词的模型为三元模型。以此类推，我们可以建立四元模型，五元模型,...一直到通用的N元模型。越往后，概率分布的计算复杂度越高。
        - N元模型的分词方法虽然很好，但是要在实际中应用也有很多问题，首先，某些生僻词，或者相邻分词联合分布在[[语料库]]中没有，概率为0。这种情况我们一般会使用[[拉普拉斯平滑]]，即给它一个较小的概率值，这个方法在[[朴素贝叶斯算法]]原理小结也有讲到。第二个问题是如果句子长，分词有很多情况，计算量也非常大，这时我们可以用下一节[[维特比算法]]来优化算法时间复杂度。
    - 常用分词工具
        - 对于文本挖掘中需要的分词功能，一般我们会用现有的工具。简单的英文分词不需要任何工具，通过空格和标点符号就可以分词了，而进一步的英文分词推荐使用nltk。对于中文分词，则推荐用[[结巴分词]]（jieba）。这些工具使用都很简单。你的分词没有特别的需求直接使用这些分词工具就可以了。
- [ 如何在自己电脑上实现大量PDF文件的[[全文搜索]]？ - 知乎 ](https://www.zhihu.com/question/19787796)
    - “[[创造性思维]]同时需要两项前提：1）焦虑感不可太高，2）压力不可消失。 人这种生物体，或许因为神经元传导速度远远低于电子线路，我早就写过，如果不能持续思考那就什么思想也得不到，于是普遍忙碌的时代总是成为思想史记录里思想最贫困的时代。感谢目前的政治压力，让我从忙碌时代稍许退出，有了足够长的思考时间。思考不是持续想着所思的题目，而是，呵呵，叔本华描写过（我几十年前引述过），如同在日本花园里散步，曲径通幽，徘徊往复，让思绪像科隆郊区的公园森林里雨后的蘑菇那样铺开。然后，最近的一个 youtube 视频的标题是，为什么我们最富于原创的思想常常在洗澡时冒出来？”
- 我目前还没学会使用如何利用python来调用中文分词工具。#[[问题]]
- [[中文分词方法]]#[[问题]]
