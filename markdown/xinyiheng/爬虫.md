- [[playwright]]
    - 微软开源最强Python自动化神器Playwright！不用写一行代码！
      via[微软开源最强Python自动化神器Playwright！不用写一行代码！ - 知乎](https://zhuanlan.zhihu.com/p/336679365)
      [[20201224]] 下午5:37
- 获取数据的[[爬虫]]
  via[附加案例二：用于获取数据的爬虫 - 少数派](https://sspai.com/post/63900)
  [[20201225]] 上午8:48
    - 爬虫的实现方式
    - ^^模式匹配。^^即对固定的句式或固定字词组合的文本内容进行提取，提到模式匹配就不得不提到「[[正则表达式]]」（Regular Expression），专门用于对固定或特定组合文本进行匹配或提取的技术。
        - 正则表达式做爬虫是在是太麻烦了。对于从渲染的 HTML 页面中提取标签节点的内容，最好的方式永远都不是正则，而是 HTML 特有的解析器。
          via[附加案例二：用于获取数据的爬虫 - 少数派](https://sspai.com/post/63900)
          [[20201225]] 上午9:07
        - 
    - ^^HTML 解析^^。这主要涉及到[[前端程序员]]领域的内容，如果你有过一些前端的基础（即 HTML、CSS 和 JavaScript），那么 90% 的爬虫对于你来说学起来简直就是易如反掌。由于大部分数据都是通过网页的形式（App 是特殊的一类，这里主要涉及的是 PC 端）来给用户进行内容展示，因此在 HTML 树中就存在着许多我们想要获取的数据，当然这样的数据是散落的，就需要特定的解析器来进行解析。
      
    - ^^直接获取数据^^。现在大部分网页呈现的内容，都是通过特定的方式由后端向前端传输数据，前端接收进行处理后渲染到页面中。而数据只要不是涉及到个人的敏感信息（如密码、校验令牌等），通常情况下都是「明文」（即直接暴露）形式传输。所以我们只要找到连同前后端的「数据桥梁」，我们就可以直接从中获取到想要的数据。而这个数据桥梁现在流行的方式主要是以[[API]]为入口，以 [[json]] 为载体。
      via[附加案例二：用于获取数据的爬虫 - 少数派](https://sspai.com/post/63900)
      [[20201225]] 上午8:49
- python中常用的爬虫工具
    - [[requests]]：Python 下载量一直很靠前的网络请求库，主要用于发起网络请求。该库的作者及其代码算是 Python 圈里公认写得好的模范，以「For Human」的口号著称，很多库都受到其影响。但它本身不具备解析功能，仅仅只是发起网络请求并获得响应内容。
      
      [[BeautifulSoup]]：人称「美丽汤」，主要用于对请求得到的 HTML 响应内容进行解析，通常和 requests 库搭配使用。它的优点在于带有「For Human」的味道，无论是在使用方法、还是节点属性，都十分符合直觉且容易上手；但它最突出的地方在于能够将一些不规范的 HTML 网页内容（如缺少闭合的 Tag 标签）转换成规范的 HTML 文档。
      
      [[lxml]]：一个高效的 XML 格式解析库，XML 文件和 HTML 文件都是标记语言，二者有着一定的联系；它和 BeautifulSoup 一样都是对响应内容进行解析，由于该库底层是基于 C 语言进行构建，所以解析速度十分迅速；同时又支持 XPath 语法，方便我们进一步提取节点元素的属性。
      
      [[Selenium]]：Selenium 最初并不是用于爬虫，因为它能模拟我们直接打开网页、点击、跳转等多个操作，所以往往都是来进行 Web 自动化测试。但正如我在前面提及爬虫的本质就是模拟人的浏览行为，所以 Selenium 的这些操作完全就是契合了这一说法，因此也渐渐被大家用于作为打开网页然后提取内容的方式之一了。不过 Selenium 需要驱动器加持，且下载驱动器的官网是 404，有兴趣的朋友可以自行了解。
      
      [[PyQuery]]：类似于 JavaScript 前端的[[ jQuery ]]类库，使用这个库可能需要稍微有点或了解一下 jQuery 的使用方法，也就是说需要了解一些前端的知识。如果你本身具备一些前端的基础，那么 PyQuery 上手不是很困难。
      
      [[Scrapy]]：Python 爬虫生态圈里的老大哥，能提供一栈式爬虫解决方案（从获取数据、解析数据到存储等）。Scrapy 框架可以算得上是 Python 爬虫生态圈里的大杀器或重型火炮，通常有一定规模的 Python 爬虫都会以 Scrapy 为基础进行构建。但它的上手门槛并不低，对新手并不算是友好（大框架的通病），等有了一定基础之后再去学会更好。
      via[附加案例二：用于获取数据的爬虫 - 少数派](https://sspai.com/post/63900)
      [[20201225]] 上午8:52
- 爬虫的上限和下限都可以无限延伸，下限低的可以直接请求并解析，上限高的还要通过解决一系列的反爬措施才能到达胜利彼岸。所以倘若想要提高我们的爬取技术，那么必不可避免的就要去了解前端的东西，比如在本章内容里我多次提到了对 HTML 的理解和基础；与此相对的，还要了解有关于网络请求 [[HTTP 协议]]的相关知识。
  via[附加案例二：用于获取数据的爬虫 - 少数派](https://sspai.com/post/63900)
  [[20201225]] 上午9:11
- [[web scraper]]
    - 
